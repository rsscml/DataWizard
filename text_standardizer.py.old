"""
Text Standardization and Entity Resolution Module
=================================================

This module provides functionality to standardize text/categorical data in Excel and CSV files
by identifying and resolving entity variants (typos, different spellings, etc.).

Features:
- Automatic detection of categorical columns suitable for standardization
- Fuzzy matching to identify variants
- Phonetic matching for names
- Configurable thresholds
- Detailed standardization reports
- Multi-worksheet support
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Union
from collections import Counter
import logging
from fuzzywuzzy import fuzz, process
import jellyfish
import re

logger = logging.getLogger(__name__)

class ColumnStandardizationResult:
    """Container for column standardization results"""
    def __init__(self, column_name: str):
        self.column_name = column_name
        self.original_unique_count = 0
        self.final_unique_count = 0
        self.variants_found = 0
        self.mappings = {}  # original -> standardized
        self.frequency_changes = {}  # standardized value -> count increase
        self.standardization_applied = False
        
class DatasetStandardizationResult:
    """Container for complete dataset standardization results"""
    def __init__(self, dataset_name: str):
        self.dataset_name = dataset_name
        self.columns_analyzed = 0
        self.columns_standardized = 0
        self.total_variants_resolved = 0
        self.column_results = {}  # column_name -> ColumnStandardizationResult
        self.processing_time = 0
        self.success = True
        self.errors = []

class TextStandardizer:
    """Main class for text standardization and entity resolution"""
    
    def __init__(self, 
                 similarity_threshold: int = 85,
                 min_frequency: int = 2,
                 max_unique_ratio: float = 0.5,
                 min_column_size: int = 5):
        """
        Initialize the standardizer with configurable parameters.
        
        Args:
            similarity_threshold: Minimum similarity score (0-100) for fuzzy matching
            min_frequency: Minimum occurrences for a value to be considered as standard
            max_unique_ratio: Maximum ratio of unique values to total rows for a column to be considered
            min_column_size: Minimum number of non-null values in a column to process
        """
        self.similarity_threshold = similarity_threshold
        self.min_frequency = min_frequency
        self.max_unique_ratio = max_unique_ratio
        self.min_column_size = min_column_size
        self.standardization_history = {}
        
    def should_standardize_column(self, series: pd.Series, column_name: str) -> Tuple[bool, str]:
        """
        Determine if a column should be standardized based on its characteristics.
        
        Returns:
            (should_standardize, reason)
        """
        # Skip if column is mostly null
        non_null_count = series.notna().sum()
        if non_null_count < self.min_column_size:
            return False, f"Too few non-null values ({non_null_count})"
        
        # Skip numeric columns
        if pd.api.types.is_numeric_dtype(series):
            # Check if it might be a categorical numeric column
            unique_values = series.dropna().unique()
            if len(unique_values) > 20:  # Likely continuous numeric
                return False, "Numeric column with many unique values"
            # Could be categorical numeric (like codes), continue checking
        
        # Skip datetime columns
        if pd.api.types.is_datetime64_any_dtype(series):
            return False, "Datetime column"
        
        # Check unique value ratio
        unique_count = series.nunique()
        unique_ratio = unique_count / len(series)
        
        if unique_ratio > self.max_unique_ratio:
            return False, f"Too many unique values ({unique_count}/{len(series)} = {unique_ratio:.2%})"
        
        if unique_count < 2:
            return False, "Only one unique value"
        
        # Good candidate for standardization
        return True, f"Categorical column with {unique_count} unique values"
    
    def preprocess_text(self, text: Any) -> str:
        """Clean and preprocess text for comparison"""
        if pd.isna(text):
            return ''
        
        # Convert to string
        text = str(text).strip()
        
        # Normalize whitespace
        text = ' '.join(text.split())
        
        # Don't lowercase everything - case might be important for some entities
        # But create a lowercase version for comparison
        return text
    
    def find_variants_fuzzy(self, values: List[Any], column_name: str) -> Dict[Any, Any]:
        """
        Find variants using fuzzy string matching.
        
        Returns:
            Dictionary mapping variants to their standard form
        """
        # Count frequencies
        value_counts = Counter(values)
        
        # Filter out None/NaN values
        valid_values = [v for v in value_counts.keys() if pd.notna(v)]
        if not valid_values:
            return {}
        
        # Sort by frequency (most common first)
        sorted_values = sorted(valid_values, key=lambda x: value_counts[x], reverse=True)
        
        # Build mapping
        mapping = {}
        already_mapped = set()
        
        for standard_value in sorted_values:
            if standard_value in already_mapped:
                continue
            
            # This value maps to itself
            mapping[standard_value] = standard_value
            already_mapped.add(standard_value)
            
            # Preprocess for comparison
            standard_processed = self.preprocess_text(standard_value).lower()
            
            # Find similar values
            for candidate_value in sorted_values:
                if candidate_value in already_mapped or candidate_value == standard_value:
                    continue
                
                candidate_processed = self.preprocess_text(candidate_value).lower()
                
                # Calculate different similarity metrics
                similarities = []
                
                # Basic ratio
                basic_ratio = fuzz.ratio(standard_processed, candidate_processed)
                similarities.append(basic_ratio)
                
                # Token sort ratio (good for reordered words)
                token_sort = fuzz.token_sort_ratio(standard_processed, candidate_processed)
                similarities.append(token_sort)
                
                # Partial ratio (good for substring matches)
                if len(standard_processed) > 3 and len(candidate_processed) > 3:
                    partial = fuzz.partial_ratio(standard_processed, candidate_processed)
                    similarities.append(partial * 0.9)  # Weight slightly lower
                
                # Use maximum similarity
                max_similarity = max(similarities)
                
                # Check if similar enough
                if max_similarity >= self.similarity_threshold:
                    # Additional checks for safety
                    if self._is_safe_to_merge(standard_value, candidate_value, value_counts):
                        mapping[candidate_value] = standard_value
                        already_mapped.add(candidate_value)
                        logger.debug(f"Column '{column_name}': '{candidate_value}' -> '{standard_value}' (similarity: {max_similarity})")
        
        return mapping
    
    def find_variants_phonetic(self, values: List[Any], column_name: str) -> Dict[Any, Any]:
        """
        Find variants using phonetic encoding (good for names).
        
        Returns:
            Dictionary mapping variants to their standard form
        """
        # Count frequencies
        value_counts = Counter(values)
        
        # Filter out None/NaN values
        valid_values = [v for v in value_counts.keys() if pd.notna(v)]
        if not valid_values:
            return {}
        
        # Group by phonetic encoding
        phonetic_groups = {}
        
        for value in valid_values:
            try:
                # Use Metaphone for phonetic encoding
                phonetic_code = jellyfish.metaphone(str(value))
                
                if phonetic_code not in phonetic_groups:
                    phonetic_groups[phonetic_code] = []
                phonetic_groups[phonetic_code].append(value)
            except:
                # If phonetic encoding fails, treat as unique
                continue
        
        # Build mapping
        mapping = {}
        
        for phonetic_code, group_values in phonetic_groups.items():
            if len(group_values) <= 1:
                # No variants in this phonetic group
                mapping[group_values[0]] = group_values[0]
            else:
                # Choose the most frequent value as standard
                group_values_sorted = sorted(group_values, 
                                            key=lambda x: value_counts[x], 
                                            reverse=True)
                standard_value = group_values_sorted[0]
                
                for value in group_values:
                    mapping[value] = standard_value
                    if value != standard_value:
                        logger.debug(f"Column '{column_name}': '{value}' -> '{standard_value}' (phonetic match)")
        
        return mapping
    
    def _is_safe_to_merge(self, standard_value: Any, candidate_value: Any, 
                          value_counts: Counter) -> bool:
        """
        Additional safety checks before merging two values.
        
        Returns:
            True if safe to merge, False otherwise
        """
        standard_str = str(standard_value)
        candidate_str = str(candidate_value)
        
        # Don't merge if one is significantly longer (might be different entity)
        len_ratio = len(candidate_str) / len(standard_str) if len(standard_str) > 0 else 0
        if len_ratio > 2.5 or len_ratio < 0.4:
            return False
        
        # Don't merge if they start with different letters/numbers (unless very similar)
        if standard_str and candidate_str:
            if standard_str[0].lower() != candidate_str[0].lower():
                # Check if the similarity is very high
                similarity = fuzz.ratio(standard_str.lower(), candidate_str.lower())
                if similarity < 90:
                    return False
        
        # Check for common false positive patterns
        # Don't merge numbers that are different
        try:
            std_num = float(standard_str)
            cand_num = float(candidate_str)
            if std_num != cand_num:
                return False
        except ValueError:
            pass  # Not numbers, continue
        
        return True
    
    def standardize_column(self, series: pd.Series, column_name: str, 
                          method: str = 'fuzzy') -> Tuple[pd.Series, ColumnStandardizationResult]:
        """
        Standardize a single column.
        
        Args:
            series: Pandas Series to standardize
            column_name: Name of the column
            method: 'fuzzy', 'phonetic', or 'combined'
            
        Returns:
            (standardized_series, result)
        """
        result = ColumnStandardizationResult(column_name)
        result.original_unique_count = series.nunique()
        
        # Check if column should be standardized
        should_standardize, reason = self.should_standardize_column(series, column_name)
        
        if not should_standardize:
            logger.info(f"Skipping column '{column_name}': {reason}")
            result.standardization_applied = False
            return series.copy(), result
        
        logger.info(f"Standardizing column '{column_name}': {reason}")
        
        # Get unique values
        unique_values = series.dropna().tolist()
        
        # Find variants based on method
        if method == 'fuzzy':
            mapping = self.find_variants_fuzzy(unique_values, column_name)
        elif method == 'phonetic':
            mapping = self.find_variants_phonetic(unique_values, column_name)
        else:  # combined
            fuzzy_mapping = self.find_variants_fuzzy(unique_values, column_name)
            phonetic_mapping = self.find_variants_phonetic(unique_values, column_name)
            
            # Combine mappings (fuzzy takes precedence)
            mapping = phonetic_mapping.copy()
            mapping.update(fuzzy_mapping)
        
        # Count actual changes
        changes = {k: v for k, v in mapping.items() if k != v}
        
        if not changes:
            logger.info(f"No variants found in column '{column_name}'")
            result.standardization_applied = False
            return series.copy(), result
        
        # Apply mapping
        standardized_series = series.map(lambda x: mapping.get(x, x))
        
        # Update result
        result.final_unique_count = standardized_series.nunique()
        result.variants_found = len(changes)
        result.mappings = changes
        result.standardization_applied = True
        
        # Calculate frequency changes
        original_counts = series.value_counts()
        standardized_counts = standardized_series.value_counts()
        
        for standard_value in set(changes.values()):
            original_count = original_counts.get(standard_value, 0)
            new_count = standardized_counts.get(standard_value, 0)
            result.frequency_changes[standard_value] = new_count - original_count
        
        logger.info(f"Column '{column_name}': Reduced {result.original_unique_count} unique values to {result.final_unique_count} ({result.variants_found} variants resolved)")
        
        return standardized_series, result
    
    def standardize_dataframe(self, df: pd.DataFrame, dataset_name: str = "dataset",
                             method: str = 'fuzzy',
                             columns_to_standardize: Optional[List[str]] = None) -> Tuple[pd.DataFrame, DatasetStandardizationResult]:
        """
        Standardize all suitable columns in a DataFrame.
        
        Args:
            df: DataFrame to standardize
            dataset_name: Name for reporting
            method: Standardization method ('fuzzy', 'phonetic', or 'combined')
            columns_to_standardize: Specific columns to standardize (None = auto-detect)
            
        Returns:
            (standardized_df, result)
        """
        import time
        start_time = time.time()
        
        result = DatasetStandardizationResult(dataset_name)
        standardized_df = df.copy()
        
        # Determine columns to process
        if columns_to_standardize:
            columns = [col for col in columns_to_standardize if col in df.columns]
        else:
            # Auto-detect suitable columns
            columns = df.columns.tolist()
        
        result.columns_analyzed = len(columns)
        
        # Process each column
        for column in columns:
            try:
                standardized_series, col_result = self.standardize_column(
                    df[column], column, method
                )
                
                if col_result.standardization_applied:
                    standardized_df[column] = standardized_series
                    result.columns_standardized += 1
                    result.total_variants_resolved += col_result.variants_found
                
                result.column_results[column] = col_result
                
            except Exception as e:
                logger.error(f"Error standardizing column '{column}': {e}")
                result.errors.append(f"Column '{column}': {str(e)}")
        
        result.processing_time = time.time() - start_time
        result.success = len(result.errors) == 0
        
        logger.info(f"Dataset '{dataset_name}': Standardized {result.columns_standardized}/{result.columns_analyzed} columns, resolved {result.total_variants_resolved} variants in {result.processing_time:.2f}s")
        
        return standardized_df, result
    
    def standardize_worksheet_dict(self, worksheets: Dict[str, pd.DataFrame],
                                  method: str = 'fuzzy') -> Tuple[Dict[str, pd.DataFrame], Dict[str, DatasetStandardizationResult]]:
        """
        Standardize multiple worksheets.
        
        Args:
            worksheets: Dictionary of worksheet_name -> DataFrame
            method: Standardization method
            
        Returns:
            (standardized_worksheets, results)
        """
        standardized_worksheets = {}
        results = {}
        
        for worksheet_name, df in worksheets.items():
            logger.info(f"Processing worksheet: {worksheet_name}")
            standardized_df, result = self.standardize_dataframe(
                df, worksheet_name, method
            )
            standardized_worksheets[worksheet_name] = standardized_df
            results[worksheet_name] = result
        
        return standardized_worksheets, results
    
    def generate_standardization_report(self, results: Union[DatasetStandardizationResult, Dict[str, DatasetStandardizationResult]]) -> str:
        """
        Generate a human-readable standardization report.
        
        Args:
            results: Single result or dictionary of results
            
        Returns:
            Formatted report string
        """
        report_lines = []
        report_lines.append("=" * 60)
        report_lines.append("TEXT STANDARDIZATION REPORT")
        report_lines.append("=" * 60)
        
        # Handle single result or multiple results
        if isinstance(results, DatasetStandardizationResult):
            results = {"Dataset": results}
        
        total_variants = 0
        total_columns_standardized = 0
        
        for dataset_name, result in results.items():
            report_lines.append(f"\nDataset: {dataset_name}")
            report_lines.append("-" * 40)
            
            if not result.success:
                report_lines.append(f"âš ï¸ Errors occurred: {', '.join(result.errors)}")
            
            report_lines.append(f"Columns analyzed: {result.columns_analyzed}")
            report_lines.append(f"Columns standardized: {result.columns_standardized}")
            report_lines.append(f"Total variants resolved: {result.total_variants_resolved}")
            report_lines.append(f"Processing time: {result.processing_time:.2f}s")
            
            if result.columns_standardized > 0:
                report_lines.append("\nStandardized columns:")
                
                for col_name, col_result in result.column_results.items():
                    if col_result.standardization_applied:
                        report_lines.append(f"\n  ðŸ“Š {col_name}:")
                        report_lines.append(f"     Original unique values: {col_result.original_unique_count}")
                        report_lines.append(f"     Final unique values: {col_result.final_unique_count}")
                        report_lines.append(f"     Variants resolved: {col_result.variants_found}")
                        
                        # Show some example mappings
                        if col_result.mappings:
                            report_lines.append("     Sample standardizations:")
                            for original, standard in list(col_result.mappings.items())[:5]:
                                report_lines.append(f"       '{original}' â†’ '{standard}'")
                            
                            if len(col_result.mappings) > 5:
                                report_lines.append(f"       ... and {len(col_result.mappings) - 5} more")
            
            total_variants += result.total_variants_resolved
            total_columns_standardized += result.columns_standardized
        
        # Summary
        report_lines.append("\n" + "=" * 60)
        report_lines.append("SUMMARY")
        report_lines.append("=" * 60)
        report_lines.append(f"Total datasets processed: {len(results)}")
        report_lines.append(f"Total columns standardized: {total_columns_standardized}")
        report_lines.append(f"Total variants resolved: {total_variants}")
        
        return "\n".join(report_lines)
    
    def get_standardization_summary(self, results: Union[DatasetStandardizationResult, Dict[str, DatasetStandardizationResult]]) -> Dict[str, Any]:
        """
        Get a summary of standardization results as a dictionary.
        
        Returns:
            Dictionary with summary statistics
        """
        if isinstance(results, DatasetStandardizationResult):
            results = {"Dataset": results}
        
        summary = {
            'datasets_processed': len(results),
            'total_columns_analyzed': sum(r.columns_analyzed for r in results.values()),
            'total_columns_standardized': sum(r.columns_standardized for r in results.values()),
            'total_variants_resolved': sum(r.total_variants_resolved for r in results.values()),
            'total_processing_time': sum(r.processing_time for r in results.values()),
            'success': all(r.success for r in results.values()),
            'datasets': {}
        }
        
        for dataset_name, result in results.items():
            summary['datasets'][dataset_name] = {
                'columns_standardized': result.columns_standardized,
                'variants_resolved': result.total_variants_resolved,
                'standardized_columns': [col for col, res in result.column_results.items() 
                                        if res.standardization_applied]
            }
        
        return summary

# Convenience functions for easy integration
def standardize_data(data: Union[pd.DataFrame, Dict[str, pd.DataFrame]], 
                     similarity_threshold: int = 85,
                     method: str = 'fuzzy',
                     columns_to_standardize: Optional[List[str]] = None,
                     verbose: bool = True) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:
    """
    Convenience function to standardize data with minimal configuration.
    
    Args:
        data: DataFrame or dictionary of DataFrames
        similarity_threshold: Fuzzy matching threshold (0-100)
        method: 'fuzzy', 'phonetic', or 'combined'
        columns_to_standardize: Specific columns to process (None = auto)
        verbose: Whether to print report
        
    Returns:
        Standardized data in same format as input
    """
    standardizer = TextStandardizer(similarity_threshold=similarity_threshold)
    
    if isinstance(data, pd.DataFrame):
        standardized_df, result = standardizer.standardize_dataframe(
            data, "data", method, columns_to_standardize
        )
        
        if verbose:
            print(standardizer.generate_standardization_report(result))
        
        return standardized_df
    else:
        standardized_worksheets, results = standardizer.standardize_worksheet_dict(
            data, method
        )
        
        if verbose:
            print(standardizer.generate_standardization_report(results))
        
        return standardized_worksheets