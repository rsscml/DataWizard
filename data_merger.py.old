"""
Data Merger Module for Multi-Dataset Processing
===============================================

This module handles merging of multiple datasets (from CSV files or Excel worksheets)
based on exact column matches, with comprehensive duplicate handling.

Key Features:
- Exact column matching only (no pattern matching)
- Duplicate removal within individual datasets
- Intelligent merging strategies
- Final duplicate cleanup
- Preservation of data integrity

Author: Assistant
Created for: Flask Excel Analytics App
"""

import pandas as pd
import numpy as np
import os
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any, Union
import logging

logger = logging.getLogger(__name__)

class DataMergeResult:
    """Container for merge operation results"""
    def __init__(self):
        self.success = False
        self.merged_df = None
        self.merge_strategy_used = None
        self.common_columns = []
        self.datasets_included = []
        self.datasets_excluded = []
        self.duplicate_stats = {}
        self.merge_summary = ""
        self.warnings = []
        self.errors = []

class DatasetDuplicateInfo:
    """Container for duplicate information"""
    def __init__(self, dataset_name: str):
        self.dataset_name = dataset_name
        self.original_rows = 0
        self.unique_rows = 0
        self.duplicates_removed = 0
        self.duplicate_columns_used = []
        self.strategy_used = ""

def remove_duplicates_from_dataset(df: pd.DataFrame, 
                                  dataset_name: str = "dataset",
                                  strategy: str = 'first',
                                  subset_columns: Optional[List[str]] = None) -> Tuple[pd.DataFrame, DatasetDuplicateInfo]:
    """
    Remove duplicates from a single dataset with detailed reporting.
    
    Args:
        df: DataFrame to deduplicate
        dataset_name: Name of the dataset for reporting
        strategy: 'first', 'last', or 'strict' (strict removes all occurrences of duplicates)
        subset_columns: Specific columns to consider for duplicate detection (None = all columns)
    
    Returns:
        (cleaned_df, duplicate_info): Cleaned DataFrame and information about duplicates removed
    """
    duplicate_info = DatasetDuplicateInfo(dataset_name)
    duplicate_info.original_rows = len(df)
    duplicate_info.strategy_used = strategy
    
    if df.empty:
        duplicate_info.unique_rows = 0
        duplicate_info.duplicates_removed = 0
        return df, duplicate_info
    
    # Determine columns to use for duplicate detection
    if subset_columns is None:
        # Use all columns
        duplicate_columns = list(df.columns)
    else:
        # Use specified columns that exist in the DataFrame
        duplicate_columns = [col for col in subset_columns if col in df.columns]
        if not duplicate_columns:
            logger.warning(f"No specified duplicate columns found in {dataset_name}, using all columns")
            duplicate_columns = list(df.columns)
    
    duplicate_info.duplicate_columns_used = duplicate_columns
    
    try:
        if strategy == 'strict':
            # Remove all rows that have duplicates (keeps only unique rows)
            duplicated_mask = df.duplicated(subset=duplicate_columns, keep=False)
            cleaned_df = df[~duplicated_mask].copy()
        else:
            # Keep first or last occurrence
            keep_strategy = 'first' if strategy == 'first' else 'last'
            cleaned_df = df.drop_duplicates(subset=duplicate_columns, keep=keep_strategy)
        
        duplicate_info.unique_rows = len(cleaned_df)
        duplicate_info.duplicates_removed = duplicate_info.original_rows - duplicate_info.unique_rows
        
        if duplicate_info.duplicates_removed > 0:
            logger.info(f"Removed {duplicate_info.duplicates_removed} duplicates from {dataset_name}")
        
        return cleaned_df.reset_index(drop=True), duplicate_info
        
    except Exception as e:
        logger.error(f"Error removing duplicates from {dataset_name}: {e}")
        duplicate_info.unique_rows = duplicate_info.original_rows
        duplicate_info.duplicates_removed = 0
        return df, duplicate_info

def find_common_columns(datasets_dict: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
    """
    Find exact column matches across all datasets.
    
    Args:
        datasets_dict: Dictionary of {dataset_name: DataFrame}
    
    Returns:
        Dictionary with analysis of common columns
    """
    if not datasets_dict or len(datasets_dict) < 2:
        return {
            'common_columns': [],
            'all_columns': [],
            'column_analysis': {},
            'merge_possible': False,
            'coverage_analysis': {}
        }
    
    # Get all unique columns across all datasets
    all_columns = set()
    dataset_columns = {}
    
    for name, df in datasets_dict.items():
        if df is not None and not df.empty:
            cols = set(df.columns)
            dataset_columns[name] = cols
            all_columns.update(cols)
    
    # Find columns that exist in ALL datasets
    common_columns = set(dataset_columns[list(dataset_columns.keys())[0]])
    for cols in dataset_columns.values():
        common_columns = common_columns.intersection(cols)
    
    # Analyze column coverage
    coverage_analysis = {}
    for col in all_columns:
        datasets_with_col = [name for name, cols in dataset_columns.items() if col in cols]
        coverage_analysis[col] = {
            'datasets': datasets_with_col,
            'coverage_count': len(datasets_with_col),
            'coverage_percentage': (len(datasets_with_col) / len(dataset_columns)) * 100,
            'is_common': col in common_columns
        }
    
    # Column type analysis for common columns
    column_analysis = {}
    for col in common_columns:
        col_info = {
            'data_types': {},
            'sample_values': {},
            'consistent_type': True
        }
        
        main_dtype = None
        for name, df in datasets_dict.items():
            if col in df.columns:
                dtype = str(df[col].dtype)
                sample_vals = df[col].dropna().head(3).tolist()
                
                col_info['data_types'][name] = dtype
                col_info['sample_values'][name] = [str(v) for v in sample_vals]
                
                if main_dtype is None:
                    main_dtype = dtype
                elif main_dtype != dtype:
                    col_info['consistent_type'] = False
        
        column_analysis[col] = col_info
    
    return {
        'common_columns': sorted(list(common_columns)),
        'all_columns': sorted(list(all_columns)),
        'column_analysis': column_analysis,
        'merge_possible': len(common_columns) > 0,
        'coverage_analysis': coverage_analysis,
        'dataset_count': len(datasets_dict),
        'datasets_with_data': len([name for name, df in datasets_dict.items() if df is not None and not df.empty])
    }

def merge_datasets_on_common_columns(datasets_dict: Dict[str, pd.DataFrame],
                                   merge_strategy: str = 'concatenate',
                                   remove_duplicates_first: bool = True,
                                   duplicate_strategy: str = 'first') -> DataMergeResult:
    """
    Merge multiple datasets based on common columns.
    
    Args:
        datasets_dict: Dictionary of {dataset_name: DataFrame}
        merge_strategy: 'concatenate' (stack datasets) or 'join' (join on common key column)
        remove_duplicates_first: Whether to remove duplicates from individual datasets first
        duplicate_strategy: Strategy for duplicate removal ('first', 'last', 'strict')
    
    Returns:
        DataMergeResult object with merge results and metadata
    """
    result = DataMergeResult()
    
    try:
        # Filter out empty datasets
        valid_datasets = {name: df for name, df in datasets_dict.items() 
                         if df is not None and not df.empty}
        
        if len(valid_datasets) == 0:
            result.errors.append("No valid datasets to merge")
            return result
        
        if len(valid_datasets) == 1:
            # Only one dataset - just clean it
            name, df = list(valid_datasets.items())[0]
            if remove_duplicates_first:
                cleaned_df, dup_info = remove_duplicates_from_dataset(df, name, duplicate_strategy)
                result.duplicate_stats[name] = dup_info
            else:
                cleaned_df = df.copy()
            
            result.success = True
            result.merged_df = cleaned_df
            result.merge_strategy_used = 'single_dataset'
            result.datasets_included = [name]
            result.merge_summary = f"Single dataset '{name}' processed"
            return result
        
        # Analyze common columns
        column_info = find_common_columns(valid_datasets)
        
        if not column_info['merge_possible']:
            result.errors.append("No common columns found across datasets - cannot merge")
            result.datasets_excluded = list(valid_datasets.keys())
            return result
        
        result.common_columns = column_info['common_columns']
        
        # Remove duplicates from individual datasets if requested
        cleaned_datasets = {}
        if remove_duplicates_first:
            for name, df in valid_datasets.items():
                cleaned_df, dup_info = remove_duplicates_from_dataset(
                    df, name, duplicate_strategy, subset_columns=result.common_columns
                )
                cleaned_datasets[name] = cleaned_df
                result.duplicate_stats[name] = dup_info
        else:
            cleaned_datasets = valid_datasets.copy()
        
        # Perform the merge based on strategy
        if merge_strategy == 'concatenate':
            result = _merge_by_concatenation(cleaned_datasets, result, column_info)
        elif merge_strategy == 'join':
            result = _merge_by_joining(cleaned_datasets, result, column_info)
        else:
            result.errors.append(f"Unknown merge strategy: {merge_strategy}")
            return result
        
        # Final duplicate removal from merged data
        if result.success and result.merged_df is not None:
            original_rows = len(result.merged_df)
            final_cleaned_df, final_dup_info = remove_duplicates_from_dataset(
                result.merged_df, "merged_data", duplicate_strategy, subset_columns=result.common_columns
            )
            result.merged_df = final_cleaned_df
            result.duplicate_stats['final_merge'] = final_dup_info
            
            if final_dup_info.duplicates_removed > 0:
                result.warnings.append(f"Removed {final_dup_info.duplicates_removed} duplicates from final merged data")

            # Reset template data since structure changed
            try:
                from session_manager import session_manager
                session_obj = session_manager.get_current_session()
                if session_obj.detected_template:
                    logger.info("Resetting template data due to CSV merge")
                    session_obj.detected_template = None
                    session_obj.template_validation_results = None
                    # Keep manual_template_override in case user wants to reapply manually
            except Exception as e:
                logger.warning(f"Error resetting template data after merge: {e}")
        
        return result
        
    except Exception as e:
        result.errors.append(f"Merge operation failed: {str(e)}")
        logger.error(f"Error in merge_datasets_on_common_columns: {e}")
        return result

def _merge_by_concatenation(datasets_dict: Dict[str, pd.DataFrame], 
                           result: DataMergeResult, 
                           column_info: Dict) -> DataMergeResult:
    """
    Merge datasets by concatenating them (stacking rows).
    """
    try:
        common_columns = column_info['common_columns']
        
        # Prepare datasets for concatenation
        datasets_for_concat = []
        for name, df in datasets_dict.items():
            # Use only common columns
            df_subset = df[common_columns].copy()
            # Add source identifier
            df_subset['_data_source'] = name
            datasets_for_concat.append(df_subset)
            result.datasets_included.append(name)
        
        # Concatenate all datasets
        merged_df = pd.concat(datasets_for_concat, ignore_index=True, sort=False)
        
        result.success = True
        result.merged_df = merged_df
        result.merge_strategy_used = 'concatenate'
        result.merge_summary = f"Concatenated {len(datasets_dict)} datasets on {len(common_columns)} common columns"
        
        return result
        
    except Exception as e:
        result.errors.append(f"Concatenation merge failed: {str(e)}")
        return result

def _merge_by_joining(datasets_dict: Dict[str, pd.DataFrame], 
                     result: DataMergeResult, 
                     column_info: Dict) -> DataMergeResult:
    """
    Merge datasets by joining them on a key column.
    """
    try:
        common_columns = column_info['common_columns']
        
        # For joining, we need to identify a potential key column
        # Look for columns that might serve as unique identifiers
        key_candidates = []
        for col in common_columns:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in ['id', 'key', 'code', 'number', 'ref', 'index']):
                key_candidates.append(col)
        
        if not key_candidates:
            # Fallback to concatenation if no clear key column
            result.warnings.append("No clear key column found for joining, falling back to concatenation")
            return _merge_by_concatenation(datasets_dict, result, column_info)
        
        # Use the first key candidate
        key_column = key_candidates[0]
        
        # Start with the first dataset
        dataset_names = list(datasets_dict.keys())
        merged_df = datasets_dict[dataset_names[0]].copy()
        result.datasets_included.append(dataset_names[0])
        
        # Join with subsequent datasets
        for name in dataset_names[1:]:
            df = datasets_dict[name]
            try:
                merged_df = pd.merge(
                    merged_df,
                    df,
                    on=key_column,
                    how='outer',
                    suffixes=('', f'_{name}')
                )
                result.datasets_included.append(name)
            except Exception as join_error:
                result.warnings.append(f"Failed to join dataset '{name}': {join_error}")
                result.datasets_excluded.append(name)
        
        result.success = True
        result.merged_df = merged_df
        result.merge_strategy_used = f'join_on_{key_column}'
        result.merge_summary = f"Joined {len(result.datasets_included)} datasets on column '{key_column}'"
        
        return result
        
    except Exception as e:
        result.errors.append(f"Join merge failed: {str(e)}")
        return result

def create_merged_excel_file(merged_df: pd.DataFrame,
                           output_path: str,
                           original_datasets_info: Dict[str, Any],
                           merge_result: DataMergeResult) -> Tuple[bool, str]:
    """
    Create an Excel file with the merged data and metadata.
    
    Args:
        merged_df: The merged DataFrame
        output_path: Path where to save the Excel file
        original_datasets_info: Information about original datasets
        merge_result: Results from the merge operation
    
    Returns:
        (success, message): Success flag and status message
    """
    try:
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # Main data sheet
            merged_df.to_excel(writer, sheet_name='Merged_Data', index=False)
            
            # Create metadata sheet
            metadata_rows = []
            metadata_rows.append(['Merge Operation Summary', ''])
            metadata_rows.append(['Timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S')])
            metadata_rows.append(['Merge Strategy', merge_result.merge_strategy_used])
            metadata_rows.append(['Total Datasets Processed', len(original_datasets_info)])
            metadata_rows.append(['Datasets Included', len(merge_result.datasets_included)])
            metadata_rows.append(['Datasets Excluded', len(merge_result.datasets_excluded)])
            metadata_rows.append(['Common Columns Used', len(merge_result.common_columns)])
            metadata_rows.append(['Final Data Shape', f"{merged_df.shape[0]} rows × {merged_df.shape[1]} columns"])
            metadata_rows.append(['', ''])
            
            # Common columns
            metadata_rows.append(['Common Columns:', ''])
            for col in merge_result.common_columns:
                metadata_rows.append([col, 'Used in merge'])
            metadata_rows.append(['', ''])
            
            # Datasets included
            metadata_rows.append(['Datasets Included:', ''])
            for dataset in merge_result.datasets_included:
                metadata_rows.append([dataset, 'Included'])
            
            # Datasets excluded
            if merge_result.datasets_excluded:
                metadata_rows.append(['', ''])
                metadata_rows.append(['Datasets Excluded:', ''])
                for dataset in merge_result.datasets_excluded:
                    metadata_rows.append([dataset, 'Excluded'])
            
            # Duplicate removal stats
            if merge_result.duplicate_stats:
                metadata_rows.append(['', ''])
                metadata_rows.append(['Duplicate Removal Stats:', ''])
                for dataset_name, dup_info in merge_result.duplicate_stats.items():
                    metadata_rows.append([f"{dataset_name} - Original Rows", dup_info.original_rows])
                    metadata_rows.append([f"{dataset_name} - Final Rows", dup_info.unique_rows])
                    metadata_rows.append([f"{dataset_name} - Duplicates Removed", dup_info.duplicates_removed])
            
            # Warnings and errors
            if merge_result.warnings:
                metadata_rows.append(['', ''])
                metadata_rows.append(['Warnings:', ''])
                for warning in merge_result.warnings:
                    metadata_rows.append(['Warning', warning])
            
            if merge_result.errors:
                metadata_rows.append(['', ''])
                metadata_rows.append(['Errors:', ''])
                for error in merge_result.errors:
                    metadata_rows.append(['Error', error])
            
            metadata_df = pd.DataFrame(metadata_rows, columns=['Field', 'Value'])
            metadata_df.to_excel(writer, sheet_name='Merge_Metadata', index=False)
        
        return True, f"Successfully created merged Excel file: {output_path}"
        
    except Exception as e:
        error_msg = f"Failed to create merged Excel file: {str(e)}"
        logger.error(error_msg)
        return False, error_msg

def analyze_merge_feasibility(datasets_dict: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
    """
    Analyze whether datasets can be merged and provide recommendations.
    
    Args:
        datasets_dict: Dictionary of {dataset_name: DataFrame}
    
    Returns:
        Dictionary with feasibility analysis and recommendations
    """
    analysis = {
        'feasible': False,
        'recommended_strategy': None,
        'common_columns_count': 0,
        'total_datasets': len(datasets_dict),
        'valid_datasets': 0,
        'recommendations': [],
        'warnings': [],
        'column_coverage': {},
        'size_analysis': {}
    }
    
    try:
        # Filter valid datasets
        valid_datasets = {name: df for name, df in datasets_dict.items() 
                         if df is not None and not df.empty}
        analysis['valid_datasets'] = len(valid_datasets)
        
        if len(valid_datasets) < 2:
            analysis['recommendations'].append("Need at least 2 non-empty datasets to merge")
            return analysis
        
        # Analyze columns
        column_info = find_common_columns(valid_datasets)
        analysis['common_columns_count'] = len(column_info['common_columns'])
        analysis['column_coverage'] = column_info['coverage_analysis']
        
        # Size analysis
        for name, df in valid_datasets.items():
            analysis['size_analysis'][name] = {
                'rows': len(df),
                'columns': len(df.columns),
                'memory_usage_mb': df.memory_usage(deep=True).sum() / (1024 * 1024)
            }
        
        # Determine feasibility
        if analysis['common_columns_count'] > 0:
            analysis['feasible'] = True
            
            # Recommend strategy
            common_cols = column_info['common_columns']
            key_like_cols = [col for col in common_cols 
                           if any(keyword in col.lower() for keyword in ['id', 'key', 'code', 'number', 'ref'])]
            
            if key_like_cols:
                analysis['recommended_strategy'] = 'join'
                analysis['recommendations'].append(f"JOIN strategy recommended using key column: {key_like_cols[0]}")
            else:
                analysis['recommended_strategy'] = 'concatenate'
                analysis['recommendations'].append("CONCATENATE strategy recommended (no clear key columns found)")
            
            # Additional recommendations
            total_rows = sum(info['rows'] for info in analysis['size_analysis'].values())
            if total_rows > 100000:
                analysis['warnings'].append(f"Large dataset merge ({total_rows:,} total rows) - expect longer processing time")
            
            # Check for potential memory issues
            total_memory = sum(info['memory_usage_mb'] for info in analysis['size_analysis'].values())
            if total_memory > 500:  # 500MB
                analysis['warnings'].append(f"High memory usage expected ({total_memory:.1f}MB total)")
        
        else:
            analysis['recommendations'].append("No common columns found - datasets cannot be merged")
            analysis['recommendations'].append("Consider:")
            analysis['recommendations'].append("- Renaming columns to match across datasets")
            analysis['recommendations'].append("- Processing datasets individually")
        
        return analysis
        
    except Exception as e:
        analysis['warnings'].append(f"Analysis failed: {str(e)}")
        return analysis

# Utility function for the main app
def should_merge_datasets(datasets_dict: Dict[str, pd.DataFrame], 
                         min_common_columns: int = 1) -> bool:
    """
    Quick check if datasets should be merged.
    
    Args:
        datasets_dict: Dictionary of datasets
        min_common_columns: Minimum number of common columns required for merge
    
    Returns:
        True if datasets should be merged, False otherwise
    """
    if len(datasets_dict) < 2:
        return False
    
    column_info = find_common_columns(datasets_dict)
    return column_info['merge_possible'] and len(column_info['common_columns']) >= min_common_columns